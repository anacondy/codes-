# backend code for fully python site , using AI models ( Gemini & Hugging face )   & switching between them ( use shift & enter at the same time to do so ) 

#-----------------------------------------------------------------------------------------------------------------------------------------------------------

#-----------------------------------------------------------------------------------BACKEND STARTS FROM HERE -----------------------------------------------





from flask import Flask, render_template
from flask_socketio import SocketIO, emit
import os
from dotenv import load_dotenv
import google.generativeai as genai
from openai import OpenAI
import requests
import mistralai.client
from cryptography.fernet import Fernet
import secrets
import re
import json
import time

# Load environment variables
load_dotenv()

app = Flask(__name__)
app.config['SECRET_KEY'] = secrets.token_hex(32)
socketio = SocketIO(app, cors_allowed_origins="*")

# Encryption setup
def generate_key():
    return Fernet.generate_key()

class ResponseProcessor:
    """Process and format AI responses"""

    @staticmethod
    def clean_response(text):
        """Remove special tokens and clean raw text"""
        if not text:
            return None

        # Remove common special tokens
        tokens_to_remove = [
            '<EOS>', '<PAD>', '<START>', '<END>', '[CLS]', '[SEP]', '[PAD]',
            '[UNK]', '</s>', '<s>', '<<SYS>>', '<</SYS>>', '[INST]', '[/INST]',
            '‚ñÅ', '##', '<|endoftext|>', '<|startoftext|>', '\u200b', '\xa0'
        ]

        for token in tokens_to_remove:
            text = text.replace(token, '')

        # Remove multiple spaces and newlines
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'\n{3,}', '\n\n', text)

        # Clean up markdown artifacts
        text = re.sub(r'\*{3,}', '', text)
        text = re.sub(r'#{3,}', '', text)

        return text.strip()

    @staticmethod
    def format_response(text, max_length=2000):
        """Format response with proper length handling"""
        if not text or len(text) < 3:
            return "I'm processing your request. Could you please elaborate?"

        # If response is very long, keep first few sentences
        if len(text) > max_length:
            sentences = re.split(r'(?<=[.!?])\s+', text)
            if len(sentences) > 5:  # Keep first 5 sentences
                return ' '.join(sentences[:5]) + '...'
            return text[:max_length] + '...'

        # Format paragraphs for better readability
        paragraphs = text.split('\n\n')
        formatted = []
        for para in paragraphs:
            if para.strip():
                formatted.append(f"<p>{para.strip()}</p>")
        return '\n'.join(formatted)

class AIManager:
    def __init__(self):
        # Load API keys
        self.gemini_key = os.getenv('GEMINI_API_KEY')
        self.openai_key = os.getenv('OPENAI_API_KEY')
        self.hf_token = os.getenv('HUGGINGFACE_TOKEN')
        self.mistral_key = os.getenv('MISTRAL_API_KEY')

        # Response processor
        self.processor = ResponseProcessor()

        # System prompts for better responses
        self.system_prompts = {
            'gemini': "You are a helpful AI assistant. Provide clear, detailed, and well-structured responses. Use paragraphs for complex answers and bullet points for lists.",
            'openai': "You are a helpful assistant. Provide detailed but clear responses. Use bullet points for multiple items when appropriate and paragraphs for explanations.",
            'huggingface': "Provide helpful, detailed, and comprehensive responses. Structure your answers clearly with paragraphs and lists when appropriate.",
            'mistral': "You are an expert assistant. Provide comprehensive, well-structured answers with examples when relevant. Use clear formatting with paragraphs and bullet points."
        }

        # Model configurations with increased token limits
        self.models = {
            'gemini': {
                'available': bool(self.gemini_key and len(self.gemini_key) > 10),
                'limit': 60,
                'count': 0,
                'model_name': 'gemini-1.5-flash',
                'max_tokens': 1000
            },
            'openai': {
                'available': bool(self.openai_key and len(self.openai_key) > 10),
                'limit': 20,
                'count': 0,
                'model_name': 'gpt-3.5-turbo',
                'max_tokens': 800
            },
            'huggingface': {
                'available': True,
                'limit': 100,
                'count': 0,
                'model_name': 'mistralai/Mistral-7B-Instruct-v0.2',
                'max_tokens': 600
            },
            'mistral': {
                'available': bool(self.mistral_key and len(self.mistral_key) > 10),
                'limit': 100,
                'count': 0,
                'model_name': 'mistral-small',
                'max_tokens': 1000
            }
        }

        # Print status
        for model, data in self.models.items():
            status = "‚úÖ" if data['available'] else "‚ùå"
            print(f"{status} {model.capitalize()} API: {'Available' if data['available'] else 'Not configured'}")

        # Set initial model priority: mistral > gemini > openai > huggingface
        if self.models['mistral']['available']:
            self.current_model = 'mistral'
        elif self.models['gemini']['available']:
            self.current_model = 'gemini'
        elif self.models['openai']['available']:
            self.current_model = 'openai'
        else:
            self.current_model = 'huggingface'

        print(f"üì° Starting with: {self.current_model}")

        # Encryption
        self.cipher_suite = Fernet(generate_key())

        # Initialize clients
        self._initialize_clients()

    def _initialize_clients(self):
        """Initialize API clients with proper configuration"""
        # Gemini
        if self.models['gemini']['available']:
            try:
                genai.configure(api_key=self.gemini_key)
                self.gemini_client = genai.GenerativeModel(
                    model_name=self.models['gemini']['model_name'],
                    generation_config={
                        "temperature": 0.7,
                        "top_p": 0.8,
                        "top_k": 40,
                        "max_output_tokens": self.models['gemini']['max_tokens'],
                    }
                )
            except Exception as e:
                print(f"‚ùå Gemini initialization error: {e}")
                self.models['gemini']['available'] = False

        # OpenAI
        if self.models['openai']['available']:
            try:
                self.openai_client = OpenAI(api_key=self.openai_key)
            except Exception as e:
                print(f"‚ùå OpenAI initialization error: {e}")
                self.models['openai']['available'] = False

        # Mistral
        if self.models['mistral']['available']:
            try:
                self.mistral_client = mistralai.client.MistralClient(api_key=self.mistral_key)
            except Exception as e:
                print(f"‚ùå Mistral initialization error: {e}")
                self.models['mistral']['available'] = False

    def encrypt_message(self, message):
        return self.cipher_suite.encrypt(message.encode()).decode()

    def decrypt_message(self, encrypted_message):
        return self.cipher_suite.decrypt(encrypted_message.encode()).decode()

    def craft_prompt(self, user_input, model_type):
        """Create optimized prompts for each model"""
        system_prompt = self.system_prompts.get(model_type, "")
        return f"{system_prompt}\n\nUser: {user_input}\nAssistant:"

    def query_ai(self, prompt, user_mood=None):
        """Query AI with proper formatting and retry logic"""
        response = None
        switched = False
        raw_response = None
        max_retries = 2

        # Check if need to switch due to limits
        if self.models[self.current_model]['count'] >= self.models[self.current_model]['limit']:
            switched = self.switch_model()

        print(f"\nü§ñ Using {self.current_model}")
        print(f"üìù User input: {prompt[:50]}...")

        # Craft optimized prompt
        optimized_prompt = self.craft_prompt(prompt, self.current_model)

        # Try to get response with retries
        for attempt in range(max_retries):
            try:
                if self.current_model == 'gemini':
                    raw_response = self._query_gemini(optimized_prompt)
                elif self.current_model == 'openai':
                    raw_response = self._query_openai(optimized_prompt)
                elif self.current_model == 'mistral':
                    raw_response = self._query_mistral(optimized_prompt)
                else:
                    raw_response = self._query_huggingface(optimized_prompt)

                if raw_response and len(raw_response) > 3:
                    break

            except Exception as e:
                print(f"‚ùå Attempt {attempt + 1} error: {e}")
                time.sleep(1)

        # If no response after retries, try switching models
        if not raw_response or len(raw_response) < 3:
            print(f"‚ö†Ô∏è Poor response from {self.current_model}, switching...")
            switched = self.switch_model()

            try:
                if self.current_model == 'gemini':
                    raw_response = self._query_gemini(optimized_prompt)
                elif self.current_model == 'openai':
                    raw_response = self._query_openai(optimized_prompt)
                elif self.current_model == 'mistral':
                    raw_response = self._query_mistral(optimized_prompt)
                else:
                    raw_response = self._query_huggingface(optimized_prompt)
            except Exception as e:
                print(f"‚ùå Error after switch: {e}")

        # Process the response
        if raw_response:
            try:
                cleaned = self.processor.clean_response(raw_response)
                formatted = self.processor.format_response(cleaned)
                response = formatted

                print(f"‚ú® Processed response length: {len(response)} characters")
                self.models[self.current_model]['count'] += 1
            except Exception as e:
                print(f"‚ùå Response processing error: {e}")
                response = None

        # Fallback if no response
        if not response:
            response = self._get_intelligent_fallback(prompt)

        # Create notification if switched
        notification = None
        if switched:
            notification = {
                'type': 'model_switch',
                'message': f'‚ö° MODEL SWITCHED TO {self.current_model.upper()} ‚ö°',
                'model': self.current_model
            }

        return {
            'response': response,
            'notification': notification,
            'current_model': self.current_model,
            'mood': user_mood
        }

    def _query_gemini(self, prompt):
        """Query Gemini with proper configuration"""
        if not self.gemini_key or not self.models['gemini']['available']:
            return None

        try:
            response = self.gemini_client.generate_content(prompt)
            return response.text if response and hasattr(response, 'text') else None
        except Exception as e:
            print(f"‚ùå Gemini error: {e}")
            return None

    def _query_openai(self, prompt):
        """Query OpenAI with conversation context"""
        if not self.openai_key or not self.models['openai']['available']:
            return None

        try:
            response = self.openai_client.chat.completions.create(
                model=self.models['openai']['model_name'],
                messages=[
                    {"role": "system", "content": self.system_prompts['openai']},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=self.models['openai']['max_tokens'],
                temperature=0.7,
                top_p=0.9
            )

            if response and response.choices:
                return response.choices[0].message.content
        except Exception as e:
            print(f"‚ùå OpenAI error: {e}")
            return None

    def _query_mistral(self, prompt):
        """Query Mistral API"""
        if not self.mistral_key or not self.models['mistral']['available']:
            return None

        try:
            chat_response = self.mistral_client.chat(
                model=self.models['mistral']['model_name'],
                messages=[{"role": "user", "content": prompt}],
                max_tokens=self.models['mistral']['max_tokens'],
                temperature=0.7
            )

            if chat_response.choices and len(chat_response.choices) > 0:
                return chat_response.choices[0].message.content
        except Exception as e:
            print(f"‚ùå Mistral error: {e}")
            return None

    def _query_huggingface(self, prompt):
        """Query HuggingFace with better model and auth"""
        if not self.models['huggingface']['available']:
            return None

        try:
            API_URL = f"https://api-inference.huggingface.co/models/{self.models['huggingface']['model_name']}"
            headers = {"Authorization": f"Bearer {self.hf_token}"} if self.hf_token else {}

            payload = {
                "inputs": prompt,
                "parameters": {
                    "max_new_tokens": self.models['huggingface']['max_tokens'],
                    "temperature": 0.7,
                    "top_p": 0.9
                }
            }

            response = requests.post(
                API_URL,
                headers=headers,
                json=payload,
                timeout=30
            )

            if response.status_code == 200:
                result = response.json()
                if isinstance(result, list) and len(result) > 0:
                    if isinstance(result[0], dict):
                        return result[0].get('generated_text', '')
                    return str(result[0])
                elif isinstance(result, dict):
                    return result.get('generated_text', '')
            else:
                print(f"HuggingFace error: {response.status_code} - {response.text}")

        except Exception as e:
            print(f"‚ùå HuggingFace error: {e}")

        return None

    def _get_intelligent_fallback(self, prompt):
        """Generate intelligent fallback responses"""
        prompt_lower = prompt.lower()

        # Contextual responses based on keywords
        if any(word in prompt_lower for word in ['hello', 'hi', 'hey']):
            return """<p>Hello! I'm an advanced AI assistant here to help you with detailed information.
            You can ask me about any topic, and I'll provide comprehensive answers with explanations and examples when helpful.
            What would you like to know today?</p>"""

        elif any(word in prompt_lower for word in ['how are', 'how do you']):
            return """<p>I'm functioning optimally and ready to assist you with detailed, well-researched answers.
            My knowledge covers a wide range of topics including science, technology, history, and current events.
            What specific topic or question can I help you with?</p>"""

        elif any(word in prompt_lower for word in ['what', 'explain']):
            topics = prompt.split()[:5]
            return f"""<p>I can provide a detailed explanation about {' '.join(topics)}. This topic typically includes several key aspects:</p>
            <ul>
                <li>Basic definitions and concepts</li>
                <li>Historical background or context</li>
                <li>Current applications or relevance</li>
                <li>Related topics or connected ideas</li>
            </ul>
            <p>Would you like me to cover all of these aspects or focus on something specific?</p>"""

        elif any(word in prompt_lower for word in ['why']):
            return """<p>That's an excellent question that often requires examining multiple factors. A comprehensive answer would typically include:</p>
            <ul>
                <li>Historical or scientific background</li>
                <li>Key principles or theories involved</li>
                <li>Real-world examples or case studies</li>
                <li>Potential counterarguments or alternative views</li>
            </ul>
            <p>Would you like me to provide a detailed analysis of all these aspects?</p>"""

        elif any(word in prompt_lower for word in ['help', 'assist']):
            return """<p>I can assist with a wide range of information and analysis, including:</p>
            <ul>
                <li>Detailed explanations of complex topics</li>
                <li>Step-by-step problem solving</li>
                <li>Comparative analysis of different concepts</li>
                <li>Creative brainstorming and idea generation</li>
                <li>Technical troubleshooting</li>
            </ul>
            <p>What specific area do you need help with?</p>"""

        elif '?' in prompt:
            return """<p>That's a thoughtful question that deserves a comprehensive answer. My response would typically include:</p>
            <ul>
                <li>A clear, direct answer to your question</li>
                <li>Supporting evidence or examples</li>
                <li>Relevant context or background information</li>
                <li>Potential implications or follow-up considerations</li>
            </ul>
            <p>Would you like me to provide this level of detail in my response?</p>"""

        else:
            # Generic but relevant response
            keywords = [word for word in prompt.split() if len(word) > 3][:3]
            if keywords:
                return f"""<p>The topic of {' '.join(keywords)} is quite comprehensive and typically includes several important dimensions:</p>
                <ul>
                    <li>Fundamental concepts and definitions</li>
                    <li>Historical development or scientific basis</li>
                    <li>Current applications and real-world examples</li>
                    <li>Related fields or interconnected topics</li>
                    <li>Future trends or emerging developments</li>
                </ul>
                <p>Would you like me to provide a detailed overview covering all these aspects?</p>"""
            else:
                return """<p>I'm ready to provide detailed assistance on any topic you're interested in. My responses can include:</p>
                <ul>
                    <li>Comprehensive explanations</li>
                    <li>Step-by-step analyses</li>
                    <li>Comparative studies</li>
                    <li>Practical examples</li>
                    <li>Visual information when relevant</li>
                </ul>
                <p>What specific information are you looking for?</p>"""

    def switch_model(self):
        """Switch to next available model with priority to Mistral"""
        # Define model priority order
        model_order = ['mistral', 'gemini', 'openai', 'huggingface']
        current_index = model_order.index(self.current_model) if self.current_model in model_order else 0

        for i in range(1, len(model_order)):
            next_index = (current_index + i) % len(model_order)
            next_model = model_order[next_index]

            if (self.models[next_model]['available'] and
                self.models[next_model]['count'] < self.models[next_model]['limit']):
                old_model = self.current_model
                self.current_model = next_model
                print(f"üîÑ Switched: {old_model} ‚Üí {next_model}")
                return True

        return False

# Initialize AI Manager
print("\n" + "=" * 60)
print("üöÄ NEURAL INTERFACE INITIALIZATION")
print("=" * 60)
ai_manager = AIManager()

@app.route('/')
def index():
    return render_template('index.html')

@socketio.on('connect')
def handle_connect():
    print('‚úÖ Client connected')

@socketio.on('disconnect')
def handle_disconnect():
    print('üëã Client disconnected')

@socketio.on('user_message')
def handle_message(data):
    try:
        message = data['message']
        encrypted = data.get('encrypted', False)
        mood = data.get('mood', 'normal')

        print(f"\nüì® New message: '{message[:50]}...'")

        # Detect mood
        message_lower = message.lower()
        if any(word in message_lower for word in ['cold', 'freeze', 'winter', 'chilly']):
            mood = 'cold'
        elif any(word in message_lower for word in ['warm', 'hot', 'summer', 'heat']):
            mood = 'warm'

        # Handle encryption
        if encrypted:
            try:
                message = ai_manager.decrypt_message(message)
            except:
                pass

        # Get AI response
        result = ai_manager.query_ai(message, mood)

        # Encrypt response if needed
        if encrypted and result['response']:
            try:
                result['response'] = ai_manager.encrypt_message(result['response'])
            except:
                pass

        print(f"üì§ Sending response (length: {len(result['response']) if result['response'] else 0})")
        emit('ai_response', result)

    except Exception as e:
        print(f"‚ùå Error: {e}")
        emit('ai_response', {
            'response': """<p>I'm currently processing your request. My systems are designed to provide detailed, comprehensive answers.
            Please try again or rephrase your question for a more complete response.</p>
            <p>You can ask about:</p>
            <ul>
                <li>Complex scientific concepts</li>
                <li>Historical events and analysis</li>
                <li>Technical problems and solutions</li>
                <li>Creative writing and brainstorming</li>
                <li>Comparative analysis of different topics</li>
            </ul>""",
            'notification': None,
            'current_model': ai_manager.current_model,
            'mood': 'normal'
        })

@socketio.on('generate_table')
def handle_table_generation(data):
    headers = ['Feature', 'Status', 'Performance', 'Health']
    rows = [
        ['Response Processing', '‚úì Active', '98%', '‚óä Optimal'],
        ['Token Cleaning', '‚úì Active', '95%', '‚óä Good'],
        ['Format Engine', '‚úì Active', '92%', '‚óã Normal'],
        ['Model Switching', '‚úì Active', '99%', '‚óä Optimal'],
        ['Encryption', '‚úì Active', '100%', '‚óä Optimal']
    ]
    emit('table_response', {'table': {'headers': headers, 'rows': rows}})

if __name__ == '__main__':
    print("=" * 60)
    print("üåê Server starting at: http://localhost:5000")
    print("üí° Tips:")
    print("   - Say 'cold' or 'warm' to change theme")
    print("   - Click üîê to toggle encryption")
    print("   - Models will auto-switch if one fails")
    print("   - Now with Mistral AI integration for high-quality responses")
    print("=" * 60 + "\n")

    socketio.run(app, debug=True, port=5000, allow_unsafe_werkzeug=True)




#----------------------------------------------------------------THIS CODE ENDS HERE -------------------------------------------------------------------------------------------
#-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------



#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


#-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

#----------------------------------------------------------------------FRONTEND BELOW -------------------------------------------------------------------------------------------






                    #GOOD UI , swtiches theme , when the user says ( i m ) feeling cold or warm & gives feedback when it switches models & when the encryption is on & off  


<!-- templates/index.html -->
<!-- CODE 12 - SHIFT+ENTER MODEL SWITCH INTEGRATED -->
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Neural Interface</title>
    <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@200;400;600&family=Space+Grotesk:wght@300;500&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-red: #FF4444;
            --primary-green: #44FF44;
            --primary-blue: #4444FF;
            --primary-yellow: #FFD700;
            --retro-yellow: #FFEB3B;
            --golden: #FFD700;
            --bg-dark: #0A0A0A;
            --text-light: #E0E0E0;
            --accent: #00FFFF;
        }

        /* Cold theme */
        body.cold-theme {
            --bg-dark: #001933;
            --primary-blue: #00BFFF;
            --accent: #87CEEB;
            --text-light: #E0F4FF;
        }

        /* Warm theme */
        body.warm-theme {
            --bg-dark: #331900;
            --primary-red: #FF6B6B;
            --accent: #FFA500;
            --text-light: #FFE4E1;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'JetBrains Mono', monospace;
            background: var(--bg-dark);
            color: var(--text-light);
            min-height: 100vh;
            transition: all 0.5s ease;
            position: relative;
            overflow-x: hidden;
        }

        /* Animated background */
        body::before {
            content: '';
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background:
                radial-gradient(circle at 20% 50%, var(--primary-blue) 0%, transparent 50%),
                radial-gradient(circle at 80% 80%, var(--primary-red) 0%, transparent 50%),
                radial-gradient(circle at 40% 20%, var(--primary-green) 0%, transparent 50%);
            opacity: 0.05;
            animation: pulse 10s ease-in-out infinite;
            pointer-events: none;
        }

        @keyframes pulse {
            0%, 100% { opacity: 0.05; }
            50% { opacity: 0.1; }
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 2rem;
            position: relative;
            z-index: 1;
        }

        .header {
            text-align: center;
            margin-bottom: 3rem;
            position: relative;
        }

        .header h1 {
            font-family: 'Space Grotesk', sans-serif;
            font-size: 3rem;
            font-weight: 300;
            letter-spacing: 0.2em;
            background: linear-gradient(90deg, var(--primary-red), var(--primary-green), var(--primary-blue), var(--primary-yellow));
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            animation: gradientShift 5s ease infinite;
        }

        @keyframes gradientShift {
            0%, 100% { filter: hue-rotate(0deg); }
            50% { filter: hue-rotate(180deg); }
        }

        .chat-container {
            background: rgba(255, 255, 255, 0.02);
            border: 1px solid rgba(255, 255, 255, 0.1);
            border-radius: 20px;
            padding: 2rem;
            backdrop-filter: blur(10px);
            box-shadow: 0 8px 32px rgba(0, 0, 0, 0.3);
        }

        .messages {
            height: 400px;
            overflow-y: auto;
            margin-bottom: 2rem;
            padding: 1rem;
            scroll-behavior: smooth;
        }

        .messages::-webkit-scrollbar {
            width: 8px;
        }

        .messages::-webkit-scrollbar-track {
            background: rgba(255, 255, 255, 0.05);
            border-radius: 10px;
        }

        .messages::-webkit-scrollbar-thumb {
            background: var(--accent);
            border-radius: 10px;
        }

        .message {
            margin-bottom: 1.5rem;
            animation: slideIn 0.3s ease;
        }

        @keyframes slideIn {
            from {
                opacity: 0;
                transform: translateX(-20px);
            }
            to {
                opacity: 1;
                transform: translateX(0);
            }
        }

        .message.user {
            text-align: right;
        }

        .message.ai {
            text-align: left;
        }

        .message-content {
            display: inline-block;
            padding: 1rem 1.5rem;
            border-radius: 15px;
            max-width: 70%;
            position: relative;
        }

        .message.user .message-content {
            background: linear-gradient(135deg, var(--primary-blue), var(--primary-green));
            color: white;
        }

        .message.ai .message-content {
            background: rgba(255, 255, 255, 0.05);
            border: 1px solid rgba(255, 255, 255, 0.1);
        }

        /* Rhombus bullet points */
        .bullet-point {
            display: inline-block;
            width: 8px;
            height: 8px;
            transform: rotate(45deg);
            margin-right: 10px;
            vertical-align: middle;
        }

        .bullet-red { background: var(--primary-red); }
        .bullet-green { background: var(--primary-green); }
        .bullet-blue { background: var(--primary-blue); }
        .bullet-yellow { background: var(--primary-yellow); }

        /* Highlighted text */
        .highlight {
            background: linear-gradient(90deg, var(--golden), var(--retro-yellow));
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            font-weight: 600;
        }

        .input-container {
            display: flex;
            gap: 1rem;
            position: relative;
        }

        .input-field {
            flex: 1;
            padding: 1rem 1.5rem;
            background: rgba(255, 255, 255, 0.05);
            border: 1px solid rgba(255, 255, 255, 0.1);
            border-radius: 50px;
            color: var(--text-light);
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.9rem;
            transition: all 0.3s ease;
        }

        .input-field:focus {
            outline: none;
            border-color: var(--accent);
            box-shadow: 0 0 20px rgba(0, 255, 255, 0.2);
        }

        .send-btn {
            padding: 1rem 2rem;
            background: linear-gradient(135deg, var(--primary-blue), var(--accent));
            border: none;
            border-radius: 50px;
            color: white;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.3s ease;
            text-transform: uppercase;
            letter-spacing: 0.1em;
        }

        .send-btn:hover {
            transform: translateY(-2px);
            box-shadow: 0 5px 20px rgba(0, 255, 255, 0.4);
        }

        /* COD style notification */
        .notification {
            position: fixed;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%) scale(0);
            background: rgba(0, 0, 0, 0.9);
            padding: 2rem 4rem;
            border: 2px solid var(--primary-yellow);
            border-radius: 10px;
            z-index: 1000;
            animation: notificationPop 3s ease forwards;
        }

        @keyframes notificationPop {
            0% {
                transform: translate(-50%, -50%) scale(0);
                opacity: 0;
            }
            20% {
                transform: translate(-50%, -50%) scale(1.2);
                opacity: 1;
            }
            40% {
                transform: translate(-50%, -50%) scale(1);
            }
            80% {
                transform: translate(-50%, -50%) scale(1);
                opacity: 1;
            }
            100% {
                transform: translate(-50%, -50%) scale(0.8);
                opacity: 0;
            }
        }

        .notification-text {
            color: var(--primary-yellow);
            font-size: 2rem;
            font-weight: 200;
            letter-spacing: 0.3em;
            text-align: center;
            text-transform: uppercase;
            text-shadow: 0 0 20px var(--primary-yellow);
        }

        /* Table styles */
        .data-table {
            width: 100%;
            margin: 1rem 0;
            border-collapse: separate;
            border-spacing: 0;
            background: rgba(255, 255, 255, 0.02);
            border-radius: 10px;
            overflow: hidden;
        }

        .data-table th {
            background: linear-gradient(135deg, var(--primary-blue), var(--primary-green));
            color: white;
            padding: 1rem;
            text-align: left;
            font-weight: 500;
            text-transform: uppercase;
            letter-spacing: 0.1em;
        }

        .data-table td {
            padding: 0.8rem 1rem;
            border-bottom: 1px solid rgba(255, 255, 255, 0.05);
        }

        .data-table tr:hover td {
            background: rgba(255, 255, 255, 0.05);
        }

        .status-indicator {
            display: inline-block;
            width: 10px;
            height: 10px;
            border-radius: 50%;
            margin-right: 8px;
        }

        .status-active { background: var(--primary-green); }
        .status-pending { background: var(--primary-yellow); }
        .status-error { background: var(--primary-red); }

        .encryption-toggle {
            position: fixed;
            bottom: 2rem;
            right: 2rem;
            background: rgba(255, 255, 255, 0.1);
            padding: 1rem;
            border-radius: 50%;
            cursor: pointer;
            transition: all 0.3s ease;
        }

        .encryption-toggle:hover {
            background: var(--accent);
            transform: rotate(180deg);
        }

        .model-indicator {
            position: fixed;
            top: 2rem;
            right: 2rem;
            background: rgba(255, 255, 255, 0.05);
            padding: 0.5rem 1rem;
            border-radius: 20px;
            border: 1px solid rgba(255, 255, 255, 0.1);
            font-size: 0.8rem;
            text-transform: uppercase;
            letter-spacing: 0.1em;
        }

        .model-indicator::before {
            content: '';
            display: inline-block;
            width: 8px;
            height: 8px;
            background: var(--primary-green);
            border-radius: 50%;
            margin-right: 8px;
            animation: blink 2s infinite;
        }

        @keyframes blink {
            0%, 100% { opacity: 1; }
            50% { opacity: 0.3; }
        }

        /* Loading animation */
        .typing-indicator {
            display: flex;
            gap: 4px;
            padding: 1rem;
        }

        .typing-dot {
            width: 8px;
            height: 8px;
            background: var(--accent);
            border-radius: 50%;
            animation: typing 1.4s infinite;
        }

        .typing-dot:nth-child(2) { animation-delay: 0.2s; }
        .typing-dot:nth-child(3) { animation-delay: 0.4s; }

        @keyframes typing {
            0%, 80%, 100% {
                transform: scale(1);
                opacity: 1;
            }
            40% {
                transform: scale(1.3);
                opacity: 0.5;
            }
        }

        /* Error message */
        .error-message {
            color: var(--primary-red);
            text-align: center;
            padding: 1rem;
            margin-top: 1rem;
            border: 1px solid var(--primary-red);
            border-radius: 10px;
            background: rgba(255, 68, 68, 0.1);
        }

        /* Responsive design */
        @media (max-width: 768px) {
            .container {
                padding: 1rem;
            }

            .header h1 {
                font-size: 2rem;
            }

            .message-content {
                max-width: 85%;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>NEURAL INTERFACE</h1>
        </div>

        <div class="model-indicator" id="modelIndicator">
            MODEL: GEMINI
        </div>

        <div class="chat-container">
            <div class="messages" id="messages">
                <div class="message ai">
                    <div class="message-content">
                        <span class="bullet-point bullet-green"></span>
                        <span class="highlight">Welcome to Neural Interface</span><br>
                        <span class="bullet-point bullet-blue"></span>
                        End-to-end encrypted communication active<br>
                        <span class="bullet-point bullet-yellow"></span>
                        Multiple AI models ready<br>
                        <span class="bullet-point bullet-red"></span>
                        Ask anything, receive precise insights<br>
                        <span class="bullet-point bullet-green"></span>
                        Press <strong>SHIFT + ENTER</strong> anytime to switch AI model
                    </div>
                </div>
            </div>

            <div class="input-container">
                <input type="text" class="input-field" id="messageInput"
                       placeholder="Enter your query..."
                       autocomplete="off">
                <button class="send-btn" onclick="sendMessage()">SEND</button>
            </div>
        </div>

        <div class="encryption-toggle" id="encryptionToggle" title="Toggle Encryption">
            üîê
        </div>
    </div>

    <script src="https://cdn.socket.io/4.5.4/socket.io.min.js"></script>
    <script>
        const socket = io();
        let encryptionEnabled = true;
        let currentMood = 'normal';
        const bulletColors = ['red', 'green', 'blue', 'yellow'];
        let bulletIndex = 0;
        let connectionError = false;

        // Initialize
        document.addEventListener('DOMContentLoaded', () => {
            const input = document.getElementById('messageInput');
            input.addEventListener('keypress', (e) => {
                if (e.key === 'Enter') {
                    sendMessage();
                }
            });

            document.getElementById('encryptionToggle').addEventListener('click', () => {
                encryptionEnabled = !encryptionEnabled;
                document.getElementById('encryptionToggle').textContent = encryptionEnabled ? 'üîê' : 'üîì';
                showNotification(encryptionEnabled ? 'ENCRYPTION ENABLED' : 'ENCRYPTION DISABLED');
            });

            // Test connection
            socket.on('connect', () => {
                console.log('Connected to server');
                connectionError = false;
                hideError();
            });

            socket.on('disconnect', () => {
                console.log('Disconnected from server');
                connectionError = true;
                showError('Disconnected from server. Attempting to reconnect...');
            });

            socket.on('connect_error', (error) => {
                console.log('Connection error:', error);
                connectionError = true;
                showError('Error connecting to the server. Please try again.');
            });
        });

        function sendMessage() {
            if (connectionError) {
                showError('Cannot send message. Connection issue detected.');
                return;
            }

            const input = document.getElementById('messageInput');
            const message = input.value.trim();

            if (!message) return;

            // Detect mood
            detectMood(message);

            // Add user message to chat
            addMessage(message, 'user');

            // Show typing indicator
            showTypingIndicator();

            // Send to server
            socket.emit('user_message', {
                message: message,
                encrypted: encryptionEnabled,
                mood: currentMood
            });

            input.value = '';
        }

        function detectMood(message) {
            const lowerMessage = message.toLowerCase();
            if (lowerMessage.includes('cold') || lowerMessage.includes('freeze') || lowerMessage.includes('winter')) {
                currentMood = 'cold';
                document.body.className = 'cold-theme';
            } else if (lowerMessage.includes('warm') || lowerMessage.includes('hot') || lowerMessage.includes('summer')) {
                currentMood = 'warm';
                document.body.className = 'warm-theme';
            } else if (lowerMessage.includes('normal') || lowerMessage.includes('default')) {
                currentMood = 'normal';
                document.body.className = '';
            }
        }

        function addMessage(content, sender) {
            const messagesDiv = document.getElementById('messages');
            const messageDiv = document.createElement('div');
            messageDiv.className = `message ${sender}`;

            const contentDiv = document.createElement('div');
            contentDiv.className = 'message-content';

            if (sender === 'ai') {
                // Format AI response with bullet points
                const lines = content.split('\n');
                const formatted = lines.map(line => {
                    if (line.trim()) {
                        const bulletColor = bulletColors[bulletIndex % bulletColors.length];
                        bulletIndex++;
                        return `<span class="bullet-point bullet-${bulletColor}"></span>${highlightText(line)}`;
                    }
                    return '';
                }).join('<br>');
                contentDiv.innerHTML = formatted;
            } else {
                contentDiv.textContent = content;
            }

            messageDiv.appendChild(contentDiv);
            messagesDiv.appendChild(messageDiv);
            messagesDiv.scrollTop = messagesDiv.scrollHeight;
        }

        function highlightText(text) {
            // Highlight important keywords
            const keywords = ['important', 'note', 'warning', 'success', 'error', 'complete', 'done'];
            let highlighted = text;

            keywords.forEach(keyword => {
                const regex = new RegExp(`\\b${keyword}\\b`, 'gi');
                highlighted = highlighted.replace(regex, `<span class="highlight">$&</span>`);
            });

            return highlighted;
        }

        function showTypingIndicator() {
            const messagesDiv = document.getElementById('messages');
            const typingDiv = document.createElement('div');
            typingDiv.className = 'message ai';
            typingDiv.id = 'typingIndicator';

            const contentDiv = document.createElement('div');
            contentDiv.className = 'message-content typing-indicator';
            contentDiv.innerHTML = '<div class="typing-dot"></div><div class="typing-dot"></div><div class="typing-dot"></div>';

            typingDiv.appendChild(contentDiv);
            messagesDiv.appendChild(typingDiv);
            messagesDiv.scrollTop = messagesDiv.scrollHeight;
        }

        function removeTypingIndicator() {
            const typingIndicator = document.getElementById('typingIndicator');
            if (typingIndicator) {
                typingIndicator.remove();
            }
        }

        function showNotification(message) {
            const notification = document.createElement('div');
            notification.className = 'notification';
            notification.innerHTML = `<div class="notification-text">${message}</div>`;
            document.body.appendChild(notification);

            setTimeout(() => {
                notification.remove();
            }, 3000);
        }

        function updateModelIndicator(model) {
            const indicator = document.getElementById('modelIndicator');
            indicator.textContent = `MODEL: ${model.toUpperCase()}`;
            indicator.style.animation = 'none';
            setTimeout(() => {
                indicator.style.animation = '';
            }, 10);
        }

        function showError(message) {
            // Remove any existing error
            hideError();

            const errorDiv = document.createElement('div');
            errorDiv.className = 'error-message';
            errorDiv.id = 'errorMessage';
            errorDiv.textContent = message;

            document.querySelector('.chat-container').appendChild(errorDiv);
        }

        function hideError() {
            const errorDiv = document.getElementById('errorMessage');
            if (errorDiv) {
                errorDiv.remove();
            }
        }

        function createTable(data) {
            if (!data.headers || !data.rows) return '';

            let table = '<table class="data-table">';
            table += '<thead><tr>';
            data.headers.forEach(header => {
                table += `<th>${header}</th>`;
            });
            table += '</tr></thead><tbody>';

            data.rows.forEach(row => {
                table += '<tr>';
                row.forEach(cell => {
                    table += `<td>${cell}</td>`;
                });
                table += '</tr>';
            });

            table += '</tbody></table>';
            return table;
        }

        // Socket event handlers
        socket.on('ai_response', (data) => {
            removeTypingIndicator();
            hideError();

            if (data.notification) {
                showNotification(data.notification.message);
                updateModelIndicator(data.notification.model);
            }

            addMessage(data.response, 'ai');

            if (data.current_model) {
                updateModelIndicator(data.current_model);
            }
        });

        socket.on('table_response', (data) => {
            removeTypingIndicator();
            const tableHtml = createTable(data.table);

            const messagesDiv = document.getElementById('messages');
            const messageDiv = document.createElement('div');
            messageDiv.className = 'message ai';

            const contentDiv = document.createElement('div');
            contentDiv.className = 'message-content';
            contentDiv.innerHTML = tableHtml;

            messageDiv.appendChild(contentDiv);
            messagesDiv.appendChild(messageDiv);
            messagesDiv.scrollTop = messagesDiv.scrollHeight;
        });

        // Generate sample table
        function requestTable(query) {
            showTypingIndicator();
            socket.emit('generate_table', { query: query });
        }

        // Keyboard shortcuts
        document.addEventListener('keydown', (e) => {
            if (e.ctrlKey && e.key === 'e') {
                document.getElementById('encryptionToggle').click();
            }
            if (e.ctrlKey && e.key === 't') {
                requestTable('sample data');
            }
        });

        // >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
        // CODE 12 - SHIFT + ENTER TO SWITCH MODEL
        // Works globally, even outside input field
        // <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
        document.addEventListener('keydown', (e) => {
            if (e.shiftKey && e.key === 'Enter') {
                e.preventDefault(); // Prevent newline if in input
                console.log('‚å®Ô∏è Shift+Enter pressed ‚Üí Requesting model switch...');
                socket.emit('switch_model'); // Trigger backend model switch
            }
        });
        // END CODE 12
    </script>
</body>
</html>




#--------------------------------------------------      REQUIREMENTS (TXT FILE ) --------------------------------------------------------------

#requirements.txt 
Flask==2.3.3
Flask-SocketIO==5.3.4
python-socketio==5.9.0
cryptography==41.0.4
google-generativeai==0.3.0
openai==1.3.0
requests==2.31.0
python-dotenv==1.0.0


#-------------------------------------------------------- THANKS -------------------------------------------------------------------------------
